{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, LSTM, Input,  Bidirectional, Conv1D, MaxPooling1D, GRU, CuDNNLSTM, Lambda, Layer\n",
    "from keras.layers.core import Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import model_from_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import functions_train\n",
    "import functions_for_intensity_events\n",
    "from keras.regularizers import l1, l1_l2, l2\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras import backend as K\n",
    "import importlib\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, precision_score, f1_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-18 16:34:59.906 INFO in 'tensorflow'['tf_logging'] at line 115: Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "model =  elmo_model(tf.squeeze(tf.cast(['apple', 'orange', 'bannana', 'strowberry'], tf.string)), signature=\"default\", as_dict=True)\n",
    "a = sess.run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dataset\n",
    "\n",
    "columns = ['CODE_GROUP', 'DUBP_CODE_NUM', 'ID']\n",
    "data_events, list_criterias = functions_for_intensity_events.prepare_data_labels('main_ori_aero_advice_table.xlsx', columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_events['CODE_GROUP_NEW'][data_events['CODE_GROUP_NEW'] == 'NAN']  = data_events['NEW_GROUP'][data_events['CODE_GROUP_NEW'] == 'NAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "binazer = LabelBinarizer()\n",
    "criterias_group = binazer.fit_transform(data_events['CODE_GROUP_NEW'].astype(str))\n",
    "\n",
    "criterias_group = pd.DataFrame(criterias_group, columns = binazer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toc = data_events['CLEAN_TEXT'].apply(lambda x: x[:45])\n",
    "X_toc = X_toc.apply(lambda x: ' '.join(x)).tolist()\n",
    "share = int(len(X_toc)*.6)\n",
    "X_train, Y_train = X_toc[:share], criterias_group[:share]\n",
    "X_test, Y_test = X_toc[share:], criterias_group[share:]\n",
    "\n",
    "\n",
    "\n",
    "x_train_shuf, x_test_shuf, y_train_shuf, y_test_shuf = train_test_split(X_train, Y_train.values, \n",
    "                                                                        test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_shuf = np.array(x_train_shuf, dtype = str)\n",
    "x_test_shuf = np.array(x_test_shuf, dtype = str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ELMO deeppavlov**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_model = hub.Module(\"http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz\", trainable=True)\n",
    "sess = tf.Session()\n",
    "\n",
    "K.set_session(sess)\n",
    "# Initialize sessions\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 1024)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(KerasLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        # These are the 3 trainable weights for word_embedding, lstm_output1 and lstm_output2\n",
    "        self.kernel1 = self.add_weight(name='kernel1',\n",
    "                                       shape=(3,),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        # This is the bias weight\n",
    "        self.kernel2 = self.add_weight(name='kernel2',\n",
    "                                       shape=(),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(KerasLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Get all the outputs of elmo_model\n",
    "        model =  elmo_model(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)\n",
    "        \n",
    "        # Embedding activation output\n",
    "        activation1 = model[\"word_emb\"]\n",
    "        \n",
    "        # First LSTM layer output\n",
    "        activation2 = model[\"lstm_outputs1\"]\n",
    "        \n",
    "        # Second LSTM layer output\n",
    "        activation3 = model[\"lstm_outputs2\"]\n",
    "\n",
    "        activation2 = tf.reduce_mean(activation2, axis=1)\n",
    "        activation3 = tf.reduce_mean(activation3, axis=1)\n",
    "        \n",
    "        mul1 = tf.scalar_mul(self.kernel1[0], activation1)\n",
    "        mul2 = tf.scalar_mul(self.kernel1[1], activation2)\n",
    "        mul3 = tf.scalar_mul(self.kernel1[2], activation3)\n",
    "        \n",
    "        sum_vector = tf.add(mul2, mul3)\n",
    "        \n",
    "        return tf.scalar_mul(self.kernel2, sum_vector)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-18 16:10:18.219 INFO in 'tensorflow'['tf_logging'] at line 115: Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)   (None, 1024)              4         \n",
      "_________________________________________________________________\n",
      "main_output (Dense)          (None, 46)                47150     \n",
      "=================================================================\n",
      "Total params: 47,154\n",
      "Trainable params: 47,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2583 samples, validate on 1108 samples\n",
      "Epoch 1/15\n",
      "2583/2583 [==============================] - ETA: 6:14 - loss: 3.8287 - acc: 0.015 - ETA: 3:10 - loss: 3.8282 - acc: 0.015 - ETA: 2:08 - loss: 3.8276 - acc: 0.041 - ETA: 1:36 - loss: 3.8269 - acc: 0.058 - ETA: 1:18 - loss: 3.8262 - acc: 0.087 - ETA: 1:05 - loss: 3.8256 - acc: 0.106 - ETA: 56s - loss: 3.8247 - acc: 0.129 - ETA: 49s - loss: 3.8240 - acc: 0.13 - ETA: 43s - loss: 3.8231 - acc: 0.14 - ETA: 39s - loss: 3.8223 - acc: 0.14 - ETA: 35s - loss: 3.8213 - acc: 0.15 - ETA: 32s - loss: 3.8204 - acc: 0.16 - ETA: 29s - loss: 3.8195 - acc: 0.17 - ETA: 27s - loss: 3.8184 - acc: 0.17 - ETA: 25s - loss: 3.8174 - acc: 0.18 - ETA: 23s - loss: 3.8165 - acc: 0.19 - ETA: 21s - loss: 3.8155 - acc: 0.20 - ETA: 19s - loss: 3.8144 - acc: 0.21 - ETA: 18s - loss: 3.8132 - acc: 0.23 - ETA: 17s - loss: 3.8120 - acc: 0.24 - ETA: 15s - loss: 3.8110 - acc: 0.24 - ETA: 14s - loss: 3.8096 - acc: 0.25 - ETA: 13s - loss: 3.8083 - acc: 0.26 - ETA: 12s - loss: 3.8070 - acc: 0.26 - ETA: 11s - loss: 3.8054 - acc: 0.27 - ETA: 10s - loss: 3.8042 - acc: 0.27 - ETA: 9s - loss: 3.8026 - acc: 0.2824 - ETA: 8s - loss: 3.8010 - acc: 0.285 - ETA: 7s - loss: 3.7993 - acc: 0.292 - ETA: 7s - loss: 3.7975 - acc: 0.296 - ETA: 6s - loss: 3.7957 - acc: 0.301 - ETA: 5s - loss: 3.7938 - acc: 0.306 - ETA: 4s - loss: 3.7920 - acc: 0.307 - ETA: 4s - loss: 3.7901 - acc: 0.308 - ETA: 3s - loss: 3.7883 - acc: 0.312 - ETA: 2s - loss: 3.7858 - acc: 0.316 - ETA: 2s - loss: 3.7842 - acc: 0.318 - ETA: 1s - loss: 3.7820 - acc: 0.319 - ETA: 0s - loss: 3.7792 - acc: 0.323 - ETA: 0s - loss: 3.7772 - acc: 0.321 - 32s 12ms/step - loss: 3.7762 - acc: 0.3209 - val_loss: 3.6574 - val_acc: 0.4179\n",
      "Epoch 2/15\n",
      "2583/2583 [==============================] - ETA: 14s - loss: 3.6700 - acc: 0.51 - ETA: 14s - loss: 3.6598 - acc: 0.46 - ETA: 13s - loss: 3.6543 - acc: 0.44 - ETA: 13s - loss: 3.6480 - acc: 0.42 - ETA: 13s - loss: 3.6437 - acc: 0.41 - ETA: 12s - loss: 3.6331 - acc: 0.44 - ETA: 12s - loss: 3.6265 - acc: 0.43 - ETA: 12s - loss: 3.6177 - acc: 0.43 - ETA: 11s - loss: 3.6130 - acc: 0.43 - ETA: 11s - loss: 3.6049 - acc: 0.45 - ETA: 10s - loss: 3.5986 - acc: 0.45 - ETA: 10s - loss: 3.5891 - acc: 0.46 - ETA: 10s - loss: 3.5829 - acc: 0.46 - ETA: 9s - loss: 3.5764 - acc: 0.4598 - ETA: 9s - loss: 3.5703 - acc: 0.460 - ETA: 9s - loss: 3.5645 - acc: 0.457 - ETA: 8s - loss: 3.5571 - acc: 0.460 - ETA: 8s - loss: 3.5464 - acc: 0.459 - ETA: 7s - loss: 3.5417 - acc: 0.458 - ETA: 7s - loss: 3.5321 - acc: 0.460 - ETA: 7s - loss: 3.5240 - acc: 0.456 - ETA: 6s - loss: 3.5138 - acc: 0.458 - ETA: 6s - loss: 3.5058 - acc: 0.457 - ETA: 6s - loss: 3.4945 - acc: 0.464 - ETA: 5s - loss: 3.4890 - acc: 0.465 - ETA: 5s - loss: 3.4802 - acc: 0.466 - ETA: 4s - loss: 3.4700 - acc: 0.469 - ETA: 4s - loss: 3.4593 - acc: 0.472 - ETA: 4s - loss: 3.4485 - acc: 0.474 - ETA: 3s - loss: 3.4420 - acc: 0.470 - ETA: 3s - loss: 3.4342 - acc: 0.470 - ETA: 3s - loss: 3.4259 - acc: 0.470 - ETA: 2s - loss: 3.4149 - acc: 0.471 - ETA: 2s - loss: 3.4023 - acc: 0.472 - ETA: 1s - loss: 3.3894 - acc: 0.473 - ETA: 1s - loss: 3.3792 - acc: 0.470 - ETA: 1s - loss: 3.3682 - acc: 0.468 - ETA: 0s - loss: 3.3569 - acc: 0.472 - ETA: 0s - loss: 3.3473 - acc: 0.471 - ETA: 0s - loss: 3.3345 - acc: 0.471 - 22s 8ms/step - loss: 3.3291 - acc: 0.4719 - val_loss: 2.8251 - val_acc: 0.4404\n",
      "Epoch 3/15\n",
      "2583/2583 [==============================] - ETA: 15s - loss: 2.7847 - acc: 0.43 - ETA: 14s - loss: 2.8276 - acc: 0.37 - ETA: 14s - loss: 2.8130 - acc: 0.39 - ETA: 13s - loss: 2.8349 - acc: 0.39 - ETA: 13s - loss: 2.8031 - acc: 0.40 - ETA: 13s - loss: 2.7727 - acc: 0.41 - ETA: 12s - loss: 2.7420 - acc: 0.40 - ETA: 12s - loss: 2.7407 - acc: 0.40 - ETA: 11s - loss: 2.7299 - acc: 0.40 - ETA: 11s - loss: 2.7195 - acc: 0.40 - ETA: 11s - loss: 2.6924 - acc: 0.40 - ETA: 10s - loss: 2.6820 - acc: 0.39 - ETA: 10s - loss: 2.6707 - acc: 0.39 - ETA: 9s - loss: 2.6701 - acc: 0.3862 - ETA: 9s - loss: 2.6599 - acc: 0.379 - ETA: 9s - loss: 2.6495 - acc: 0.375 - ETA: 8s - loss: 2.6503 - acc: 0.367 - ETA: 8s - loss: 2.6450 - acc: 0.370 - ETA: 8s - loss: 2.6367 - acc: 0.365 - ETA: 7s - loss: 2.6234 - acc: 0.367 - ETA: 7s - loss: 2.6172 - acc: 0.363 - ETA: 6s - loss: 2.5959 - acc: 0.363 - ETA: 6s - loss: 2.5905 - acc: 0.362 - ETA: 6s - loss: 2.5884 - acc: 0.361 - ETA: 5s - loss: 2.5859 - acc: 0.361 - ETA: 5s - loss: 2.5746 - acc: 0.360 - ETA: 4s - loss: 2.5754 - acc: 0.358 - ETA: 4s - loss: 2.5697 - acc: 0.361 - ETA: 4s - loss: 2.5665 - acc: 0.358 - ETA: 3s - loss: 2.5601 - acc: 0.358 - ETA: 3s - loss: 2.5529 - acc: 0.359 - ETA: 3s - loss: 2.5418 - acc: 0.357 - ETA: 2s - loss: 2.5361 - acc: 0.354 - ETA: 2s - loss: 2.5296 - acc: 0.353 - ETA: 2s - loss: 2.5234 - acc: 0.353 - ETA: 1s - loss: 2.5181 - acc: 0.353 - ETA: 1s - loss: 2.5174 - acc: 0.350 - ETA: 0s - loss: 2.5143 - acc: 0.347 - ETA: 0s - loss: 2.5105 - acc: 0.348 - ETA: 0s - loss: 2.5077 - acc: 0.347 - 22s 8ms/step - loss: 2.5063 - acc: 0.3477 - val_loss: 2.3258 - val_acc: 0.3069\n",
      "Epoch 4/15\n",
      "2583/2583 [==============================] - ETA: 14s - loss: 2.5153 - acc: 0.25 - ETA: 14s - loss: 2.3213 - acc: 0.28 - ETA: 14s - loss: 2.2325 - acc: 0.30 - ETA: 13s - loss: 2.2549 - acc: 0.30 - ETA: 13s - loss: 2.3091 - acc: 0.31 - ETA: 12s - loss: 2.2959 - acc: 0.30 - ETA: 12s - loss: 2.2980 - acc: 0.31 - ETA: 12s - loss: 2.3368 - acc: 0.31 - ETA: 11s - loss: 2.3310 - acc: 0.31 - ETA: 11s - loss: 2.3006 - acc: 0.32 - ETA: 11s - loss: 2.3034 - acc: 0.33 - ETA: 10s - loss: 2.3231 - acc: 0.33 - ETA: 10s - loss: 2.3002 - acc: 0.33 - ETA: 9s - loss: 2.3066 - acc: 0.3393 - ETA: 9s - loss: 2.3125 - acc: 0.339 - ETA: 9s - loss: 2.2995 - acc: 0.345 - ETA: 8s - loss: 2.2832 - acc: 0.354 - ETA: 8s - loss: 2.2870 - acc: 0.355 - ETA: 8s - loss: 2.2805 - acc: 0.363 - ETA: 7s - loss: 2.2740 - acc: 0.368 - ETA: 7s - loss: 2.2887 - acc: 0.369 - ETA: 6s - loss: 2.2827 - acc: 0.373 - ETA: 6s - loss: 2.2808 - acc: 0.373 - ETA: 6s - loss: 2.2764 - acc: 0.373 - ETA: 5s - loss: 2.2705 - acc: 0.375 - ETA: 5s - loss: 2.2741 - acc: 0.373 - ETA: 5s - loss: 2.2727 - acc: 0.372 - ETA: 4s - loss: 2.2742 - acc: 0.376 - ETA: 4s - loss: 2.2666 - acc: 0.378 - ETA: 3s - loss: 2.2673 - acc: 0.381 - ETA: 3s - loss: 2.2597 - acc: 0.384 - ETA: 3s - loss: 2.2650 - acc: 0.383 - ETA: 2s - loss: 2.2574 - acc: 0.388 - ETA: 2s - loss: 2.2550 - acc: 0.387 - ETA: 2s - loss: 2.2501 - acc: 0.390 - ETA: 1s - loss: 2.2529 - acc: 0.393 - ETA: 1s - loss: 2.2517 - acc: 0.394 - ETA: 0s - loss: 2.2453 - acc: 0.399 - ETA: 0s - loss: 2.2437 - acc: 0.403 - ETA: 0s - loss: 2.2385 - acc: 0.407 - 22s 8ms/step - loss: 2.2431 - acc: 0.4077 - val_loss: 2.1724 - val_acc: 0.5226\n",
      "Epoch 5/15\n",
      "2583/2583 [==============================] - ETA: 14s - loss: 1.9813 - acc: 0.59 - ETA: 14s - loss: 1.9765 - acc: 0.58 - ETA: 13s - loss: 2.0575 - acc: 0.52 - ETA: 13s - loss: 2.0068 - acc: 0.52 - ETA: 13s - loss: 2.0432 - acc: 0.53 - ETA: 12s - loss: 2.0468 - acc: 0.54 - ETA: 12s - loss: 2.0869 - acc: 0.53 - ETA: 12s - loss: 2.0996 - acc: 0.52 - ETA: 11s - loss: 2.0864 - acc: 0.53 - ETA: 11s - loss: 2.1052 - acc: 0.53 - ETA: 10s - loss: 2.0820 - acc: 0.53 - ETA: 10s - loss: 2.0854 - acc: 0.53 - ETA: 10s - loss: 2.0838 - acc: 0.53 - ETA: 9s - loss: 2.0917 - acc: 0.5290 - ETA: 9s - loss: 2.1101 - acc: 0.526 - ETA: 9s - loss: 2.0972 - acc: 0.527 - ETA: 8s - loss: 2.0944 - acc: 0.523 - ETA: 8s - loss: 2.1077 - acc: 0.522 - ETA: 7s - loss: 2.1049 - acc: 0.523 - ETA: 7s - loss: 2.0925 - acc: 0.521 - ETA: 7s - loss: 2.0900 - acc: 0.522 - ETA: 6s - loss: 2.0857 - acc: 0.522 - ETA: 6s - loss: 2.0850 - acc: 0.525 - ETA: 6s - loss: 2.0920 - acc: 0.526 - ETA: 5s - loss: 2.0938 - acc: 0.528 - ETA: 5s - loss: 2.0977 - acc: 0.525 - ETA: 4s - loss: 2.0993 - acc: 0.524 - ETA: 4s - loss: 2.0976 - acc: 0.525 - ETA: 4s - loss: 2.0912 - acc: 0.529 - ETA: 3s - loss: 2.0940 - acc: 0.529 - ETA: 3s - loss: 2.0961 - acc: 0.529 - ETA: 3s - loss: 2.0909 - acc: 0.530 - ETA: 2s - loss: 2.0873 - acc: 0.532 - ETA: 2s - loss: 2.0851 - acc: 0.532 - ETA: 2s - loss: 2.0922 - acc: 0.534 - ETA: 1s - loss: 2.0829 - acc: 0.537 - ETA: 1s - loss: 2.0827 - acc: 0.538 - ETA: 0s - loss: 2.0882 - acc: 0.537 - ETA: 0s - loss: 2.0901 - acc: 0.537 - ETA: 0s - loss: 2.0908 - acc: 0.537 - 22s 8ms/step - loss: 2.0877 - acc: 0.5377 - val_loss: 2.0211 - val_acc: 0.5912\n",
      "Epoch 6/15\n",
      "2583/2583 [==============================] - ETA: 14s - loss: 1.9981 - acc: 0.57 - ETA: 14s - loss: 1.9444 - acc: 0.59 - ETA: 14s - loss: 1.9184 - acc: 0.59 - ETA: 13s - loss: 1.8784 - acc: 0.58 - ETA: 13s - loss: 1.9150 - acc: 0.58 - ETA: 13s - loss: 1.9157 - acc: 0.58 - ETA: 12s - loss: 1.9241 - acc: 0.58 - ETA: 12s - loss: 1.9122 - acc: 0.58 - ETA: 11s - loss: 1.9489 - acc: 0.57 - ETA: 11s - loss: 1.9298 - acc: 0.58 - ETA: 11s - loss: 1.9257 - acc: 0.58 - ETA: 10s - loss: 1.9307 - acc: 0.58 - ETA: 10s - loss: 1.9358 - acc: 0.58 - ETA: 9s - loss: 1.9771 - acc: 0.5714 - ETA: 9s - loss: 2.0095 - acc: 0.558 - ETA: 9s - loss: 1.9998 - acc: 0.559 - ETA: 8s - loss: 1.9901 - acc: 0.564 - ETA: 8s - loss: 1.9975 - acc: 0.560 - ETA: 8s - loss: 1.9903 - acc: 0.561 - ETA: 7s - loss: 1.9867 - acc: 0.560 - ETA: 7s - loss: 1.9768 - acc: 0.564 - ETA: 6s - loss: 1.9808 - acc: 0.563 - ETA: 6s - loss: 1.9710 - acc: 0.568 - ETA: 6s - loss: 1.9726 - acc: 0.569 - ETA: 5s - loss: 1.9732 - acc: 0.566 - ETA: 5s - loss: 1.9797 - acc: 0.568 - ETA: 5s - loss: 1.9796 - acc: 0.568 - ETA: 4s - loss: 1.9789 - acc: 0.564 - ETA: 4s - loss: 1.9681 - acc: 0.566 - ETA: 3s - loss: 1.9727 - acc: 0.562 - ETA: 3s - loss: 1.9806 - acc: 0.561 - ETA: 3s - loss: 1.9797 - acc: 0.562 - ETA: 2s - loss: 1.9783 - acc: 0.563 - ETA: 2s - loss: 1.9664 - acc: 0.567 - ETA: 2s - loss: 1.9544 - acc: 0.570 - ETA: 1s - loss: 1.9440 - acc: 0.572 - ETA: 1s - loss: 1.9410 - acc: 0.573 - ETA: 0s - loss: 1.9390 - acc: 0.576 - ETA: 0s - loss: 1.9282 - acc: 0.577 - ETA: 0s - loss: 1.9304 - acc: 0.574 - 22s 8ms/step - loss: 1.9292 - acc: 0.5745 - val_loss: 1.8599 - val_acc: 0.6047\n",
      "Epoch 7/15\n",
      "2583/2583 [==============================] - ETA: 14s - loss: 1.3199 - acc: 0.70 - ETA: 14s - loss: 1.5182 - acc: 0.64 - ETA: 14s - loss: 1.5636 - acc: 0.63 - ETA: 13s - loss: 1.5698 - acc: 0.62 - ETA: 13s - loss: 1.6681 - acc: 0.60 - ETA: 12s - loss: 1.7060 - acc: 0.58 - ETA: 12s - loss: 1.6900 - acc: 0.59 - ETA: 12s - loss: 1.7654 - acc: 0.58 - ETA: 11s - loss: 1.7733 - acc: 0.57 - ETA: 11s - loss: 1.7791 - acc: 0.58 - ETA: 10s - loss: 1.7930 - acc: 0.57 - ETA: 10s - loss: 1.7944 - acc: 0.58 - ETA: 10s - loss: 1.7597 - acc: 0.59 - ETA: 9s - loss: 1.7851 - acc: 0.5893 - ETA: 9s - loss: 1.7798 - acc: 0.588 - ETA: 9s - loss: 1.7798 - acc: 0.590 - ETA: 8s - loss: 1.7791 - acc: 0.590 - ETA: 8s - loss: 1.7781 - acc: 0.592 - ETA: 8s - loss: 1.7727 - acc: 0.592 - ETA: 7s - loss: 1.8015 - acc: 0.587 - ETA: 7s - loss: 1.8112 - acc: 0.588 - ETA: 6s - loss: 1.8031 - acc: 0.590 - ETA: 6s - loss: 1.7994 - acc: 0.594 - ETA: 6s - loss: 1.7917 - acc: 0.593 - ETA: 5s - loss: 1.7934 - acc: 0.595 - ETA: 5s - loss: 1.7943 - acc: 0.594 - ETA: 5s - loss: 1.7865 - acc: 0.596 - ETA: 4s - loss: 1.7796 - acc: 0.596 - ETA: 4s - loss: 1.7725 - acc: 0.595 - ETA: 3s - loss: 1.7745 - acc: 0.597 - ETA: 3s - loss: 1.7779 - acc: 0.598 - ETA: 3s - loss: 1.7789 - acc: 0.599 - ETA: 2s - loss: 1.7723 - acc: 0.597 - ETA: 2s - loss: 1.7689 - acc: 0.599 - ETA: 2s - loss: 1.7691 - acc: 0.598 - ETA: 1s - loss: 1.7629 - acc: 0.599 - ETA: 1s - loss: 1.7642 - acc: 0.600 - ETA: 0s - loss: 1.7684 - acc: 0.599 - ETA: 0s - loss: 1.7615 - acc: 0.601 - ETA: 0s - loss: 1.7543 - acc: 0.602 - 22s 8ms/step - loss: 1.7575 - acc: 0.6012 - val_loss: 1.6935 - val_acc: 0.6300\n",
      "Epoch 8/15\n",
      "2583/2583 [==============================] - ETA: 14s - loss: 1.6643 - acc: 0.56 - ETA: 14s - loss: 1.5583 - acc: 0.63 - ETA: 13s - loss: 1.5666 - acc: 0.63 - ETA: 13s - loss: 1.5673 - acc: 0.62 - ETA: 13s - loss: 1.5988 - acc: 0.61 - ETA: 12s - loss: 1.6280 - acc: 0.60 - ETA: 12s - loss: 1.6708 - acc: 0.60 - ETA: 12s - loss: 1.6857 - acc: 0.62 - ETA: 11s - loss: 1.7054 - acc: 0.60 - ETA: 11s - loss: 1.6721 - acc: 0.61 - ETA: 10s - loss: 1.6625 - acc: 0.61 - ETA: 10s - loss: 1.6341 - acc: 0.62 - ETA: 10s - loss: 1.6338 - acc: 0.62 - ETA: 9s - loss: 1.6374 - acc: 0.6272 - ETA: 9s - loss: 1.6219 - acc: 0.631 - ETA: 9s - loss: 1.6046 - acc: 0.635 - ETA: 8s - loss: 1.6083 - acc: 0.637 - ETA: 8s - loss: 1.6153 - acc: 0.638 - ETA: 7s - loss: 1.6066 - acc: 0.639 - ETA: 7s - loss: 1.6211 - acc: 0.636 - ETA: 7s - loss: 1.6301 - acc: 0.633 - ETA: 6s - loss: 1.6256 - acc: 0.632 - ETA: 6s - loss: 1.6334 - acc: 0.631 - ETA: 6s - loss: 1.6267 - acc: 0.631 - ETA: 5s - loss: 1.6143 - acc: 0.636 - ETA: 5s - loss: 1.6176 - acc: 0.635 - ETA: 4s - loss: 1.6112 - acc: 0.633 - ETA: 4s - loss: 1.6062 - acc: 0.631 - ETA: 4s - loss: 1.6003 - acc: 0.634 - ETA: 3s - loss: 1.5970 - acc: 0.635 - ETA: 3s - loss: 1.5912 - acc: 0.635 - ETA: 3s - loss: 1.5937 - acc: 0.635 - ETA: 2s - loss: 1.5946 - acc: 0.636 - ETA: 2s - loss: 1.5920 - acc: 0.637 - ETA: 1s - loss: 1.6092 - acc: 0.633 - ETA: 1s - loss: 1.6098 - acc: 0.631 - ETA: 1s - loss: 1.6021 - acc: 0.633 - ETA: 0s - loss: 1.6022 - acc: 0.631 - ETA: 0s - loss: 1.5905 - acc: 0.633 - ETA: 0s - loss: 1.5860 - acc: 0.634 - 22s 8ms/step - loss: 1.5845 - acc: 0.6353 - val_loss: 1.5326 - val_acc: 0.6688\n",
      "Epoch 9/15\n",
      "2583/2583 [==============================] - ETA: 14s - loss: 1.2015 - acc: 0.75 - ETA: 14s - loss: 1.3381 - acc: 0.65 - ETA: 13s - loss: 1.3977 - acc: 0.69 - ETA: 13s - loss: 1.3189 - acc: 0.71 - ETA: 13s - loss: 1.3262 - acc: 0.71 - ETA: 12s - loss: 1.3452 - acc: 0.70 - ETA: 12s - loss: 1.3446 - acc: 0.70 - ETA: 12s - loss: 1.3420 - acc: 0.69 - ETA: 11s - loss: 1.3177 - acc: 0.69 - ETA: 11s - loss: 1.3386 - acc: 0.69 - ETA: 10s - loss: 1.3288 - acc: 0.69 - ETA: 10s - loss: 1.3465 - acc: 0.68 - ETA: 10s - loss: 1.3382 - acc: 0.68 - ETA: 9s - loss: 1.3477 - acc: 0.6853 - ETA: 9s - loss: 1.3606 - acc: 0.681 - ETA: 9s - loss: 1.3771 - acc: 0.676 - ETA: 8s - loss: 1.3820 - acc: 0.681 - ETA: 8s - loss: 1.3952 - acc: 0.682 - ETA: 7s - loss: 1.3984 - acc: 0.680 - ETA: 7s - loss: 1.3944 - acc: 0.678 - ETA: 7s - loss: 1.3872 - acc: 0.680 - ETA: 6s - loss: 1.3858 - acc: 0.682 - ETA: 6s - loss: 1.4013 - acc: 0.675 - ETA: 6s - loss: 1.4013 - acc: 0.675 - ETA: 5s - loss: 1.3959 - acc: 0.675 - ETA: 5s - loss: 1.3927 - acc: 0.676 - ETA: 4s - loss: 1.3842 - acc: 0.677 - ETA: 4s - loss: 1.3991 - acc: 0.674 - ETA: 4s - loss: 1.3948 - acc: 0.674 - ETA: 3s - loss: 1.4017 - acc: 0.674 - ETA: 3s - loss: 1.4067 - acc: 0.672 - ETA: 3s - loss: 1.4055 - acc: 0.672 - ETA: 2s - loss: 1.4105 - acc: 0.670 - ETA: 2s - loss: 1.4123 - acc: 0.670 - ETA: 1s - loss: 1.4182 - acc: 0.671 - ETA: 1s - loss: 1.4097 - acc: 0.673 - ETA: 1s - loss: 1.4065 - acc: 0.674 - ETA: 0s - loss: 1.4078 - acc: 0.673 - ETA: 0s - loss: 1.4208 - acc: 0.673 - ETA: 0s - loss: 1.4223 - acc: 0.674 - 22s 8ms/step - loss: 1.4254 - acc: 0.6748 - val_loss: 1.3977 - val_acc: 0.6949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15\n",
      "2583/2583 [==============================] - ETA: 14s - loss: 1.2514 - acc: 0.73 - ETA: 14s - loss: 1.2947 - acc: 0.71 - ETA: 13s - loss: 1.1868 - acc: 0.72 - ETA: 13s - loss: 1.2524 - acc: 0.71 - ETA: 13s - loss: 1.2856 - acc: 0.70 - ETA: 12s - loss: 1.2852 - acc: 0.70 - ETA: 12s - loss: 1.2723 - acc: 0.70 - ETA: 12s - loss: 1.2910 - acc: 0.70 - ETA: 11s - loss: 1.2950 - acc: 0.70 - ETA: 11s - loss: 1.3057 - acc: 0.70 - ETA: 10s - loss: 1.3064 - acc: 0.70 - ETA: 10s - loss: 1.2749 - acc: 0.70 - ETA: 10s - loss: 1.2592 - acc: 0.71 - ETA: 9s - loss: 1.2854 - acc: 0.7076 - ETA: 9s - loss: 1.2959 - acc: 0.705 - ETA: 9s - loss: 1.2905 - acc: 0.706 - ETA: 8s - loss: 1.2912 - acc: 0.704 - ETA: 8s - loss: 1.2872 - acc: 0.704 - ETA: 7s - loss: 1.2908 - acc: 0.705 - ETA: 7s - loss: 1.2917 - acc: 0.706 - ETA: 7s - loss: 1.2938 - acc: 0.701 - ETA: 6s - loss: 1.2930 - acc: 0.701 - ETA: 6s - loss: 1.3079 - acc: 0.702 - ETA: 6s - loss: 1.3101 - acc: 0.703 - ETA: 5s - loss: 1.3059 - acc: 0.705 - ETA: 5s - loss: 1.2955 - acc: 0.707 - ETA: 4s - loss: 1.3003 - acc: 0.706 - ETA: 4s - loss: 1.2971 - acc: 0.707 - ETA: 4s - loss: 1.2939 - acc: 0.710 - ETA: 3s - loss: 1.2988 - acc: 0.709 - ETA: 3s - loss: 1.3057 - acc: 0.708 - ETA: 3s - loss: 1.3103 - acc: 0.709 - ETA: 2s - loss: 1.3084 - acc: 0.708 - ETA: 2s - loss: 1.2998 - acc: 0.710 - ETA: 1s - loss: 1.3070 - acc: 0.706 - ETA: 1s - loss: 1.2894 - acc: 0.710 - ETA: 1s - loss: 1.2867 - acc: 0.710 - ETA: 0s - loss: 1.2971 - acc: 0.709 - ETA: 0s - loss: 1.3000 - acc: 0.709 - ETA: 0s - loss: 1.2909 - acc: 0.711 - 22s 8ms/step - loss: 1.2909 - acc: 0.7116 - val_loss: 1.2865 - val_acc: 0.7184\n",
      "Epoch 11/15\n",
      "2583/2583 [==============================] - ETA: 14s - loss: 1.2304 - acc: 0.78 - ETA: 14s - loss: 1.1122 - acc: 0.78 - ETA: 13s - loss: 1.1775 - acc: 0.75 - ETA: 13s - loss: 1.1017 - acc: 0.77 - ETA: 13s - loss: 1.0575 - acc: 0.78 - ETA: 12s - loss: 1.0641 - acc: 0.76 - ETA: 12s - loss: 1.0481 - acc: 0.76 - ETA: 12s - loss: 1.0798 - acc: 0.76 - ETA: 11s - loss: 1.0744 - acc: 0.75 - ETA: 11s - loss: 1.1228 - acc: 0.74 - ETA: 10s - loss: 1.1602 - acc: 0.72 - ETA: 10s - loss: 1.1570 - acc: 0.72 - ETA: 10s - loss: 1.1874 - acc: 0.72 - ETA: 9s - loss: 1.1754 - acc: 0.7254 - ETA: 9s - loss: 1.1582 - acc: 0.728 - ETA: 9s - loss: 1.1379 - acc: 0.732 - ETA: 8s - loss: 1.1270 - acc: 0.736 - ETA: 8s - loss: 1.1254 - acc: 0.733 - ETA: 7s - loss: 1.1546 - acc: 0.727 - ETA: 7s - loss: 1.1568 - acc: 0.726 - ETA: 7s - loss: 1.1683 - acc: 0.725 - ETA: 6s - loss: 1.1926 - acc: 0.717 - ETA: 6s - loss: 1.1836 - acc: 0.718 - ETA: 6s - loss: 1.1891 - acc: 0.720 - ETA: 5s - loss: 1.1765 - acc: 0.725 - ETA: 5s - loss: 1.1765 - acc: 0.726 - ETA: 4s - loss: 1.1767 - acc: 0.728 - ETA: 4s - loss: 1.1791 - acc: 0.728 - ETA: 4s - loss: 1.1825 - acc: 0.726 - ETA: 3s - loss: 1.1902 - acc: 0.725 - ETA: 3s - loss: 1.1970 - acc: 0.723 - ETA: 3s - loss: 1.1965 - acc: 0.724 - ETA: 2s - loss: 1.1935 - acc: 0.726 - ETA: 2s - loss: 1.1959 - acc: 0.725 - ETA: 1s - loss: 1.1935 - acc: 0.725 - ETA: 1s - loss: 1.1900 - acc: 0.727 - ETA: 1s - loss: 1.1890 - acc: 0.725 - ETA: 0s - loss: 1.1904 - acc: 0.724 - ETA: 0s - loss: 1.1932 - acc: 0.724 - ETA: 0s - loss: 1.1856 - acc: 0.726 - 22s 8ms/step - loss: 1.1814 - acc: 0.7274 - val_loss: 1.2021 - val_acc: 0.7392\n",
      "Epoch 12/15\n",
      "2583/2583 [==============================] - ETA: 14s - loss: 1.6309 - acc: 0.65 - ETA: 14s - loss: 1.4269 - acc: 0.64 - ETA: 13s - loss: 1.2839 - acc: 0.68 - ETA: 13s - loss: 1.2332 - acc: 0.69 - ETA: 13s - loss: 1.2111 - acc: 0.70 - ETA: 12s - loss: 1.2214 - acc: 0.69 - ETA: 12s - loss: 1.1592 - acc: 0.70 - ETA: 12s - loss: 1.1300 - acc: 0.71 - ETA: 11s - loss: 1.1139 - acc: 0.71 - ETA: 11s - loss: 1.1191 - acc: 0.71 - ETA: 10s - loss: 1.1303 - acc: 0.71 - ETA: 10s - loss: 1.1558 - acc: 0.71 - ETA: 10s - loss: 1.1636 - acc: 0.71 - ETA: 9s - loss: 1.1388 - acc: 0.7165 - ETA: 9s - loss: 1.1453 - acc: 0.717 - ETA: 9s - loss: 1.1340 - acc: 0.726 - ETA: 8s - loss: 1.1182 - acc: 0.734 - ETA: 8s - loss: 1.1050 - acc: 0.736 - ETA: 7s - loss: 1.1073 - acc: 0.736 - ETA: 7s - loss: 1.1153 - acc: 0.736 - ETA: 7s - loss: 1.1041 - acc: 0.741 - ETA: 6s - loss: 1.1087 - acc: 0.739 - ETA: 6s - loss: 1.1171 - acc: 0.737 - ETA: 6s - loss: 1.1209 - acc: 0.737 - ETA: 5s - loss: 1.1214 - acc: 0.736 - ETA: 5s - loss: 1.1238 - acc: 0.736 - ETA: 4s - loss: 1.1317 - acc: 0.735 - ETA: 4s - loss: 1.1235 - acc: 0.736 - ETA: 4s - loss: 1.1178 - acc: 0.736 - ETA: 3s - loss: 1.1060 - acc: 0.740 - ETA: 3s - loss: 1.1031 - acc: 0.740 - ETA: 3s - loss: 1.0893 - acc: 0.742 - ETA: 2s - loss: 1.0885 - acc: 0.741 - ETA: 2s - loss: 1.0899 - acc: 0.741 - ETA: 1s - loss: 1.0909 - acc: 0.742 - ETA: 1s - loss: 1.0855 - acc: 0.742 - ETA: 1s - loss: 1.0760 - acc: 0.745 - ETA: 0s - loss: 1.0797 - acc: 0.745 - ETA: 0s - loss: 1.0849 - acc: 0.744 - ETA: 0s - loss: 1.0885 - acc: 0.742 - 22s 8ms/step - loss: 1.0881 - acc: 0.7425 - val_loss: 1.1372 - val_acc: 0.7482\n",
      "Epoch 13/15\n",
      "2583/2583 [==============================] - ETA: 14s - loss: 0.9237 - acc: 0.81 - ETA: 14s - loss: 0.9503 - acc: 0.78 - ETA: 13s - loss: 0.9547 - acc: 0.77 - ETA: 13s - loss: 1.0131 - acc: 0.76 - ETA: 13s - loss: 0.9885 - acc: 0.76 - ETA: 12s - loss: 1.0652 - acc: 0.75 - ETA: 12s - loss: 1.0671 - acc: 0.75 - ETA: 12s - loss: 1.0589 - acc: 0.75 - ETA: 11s - loss: 1.0344 - acc: 0.76 - ETA: 11s - loss: 1.0195 - acc: 0.76 - ETA: 10s - loss: 1.0065 - acc: 0.76 - ETA: 10s - loss: 0.9933 - acc: 0.76 - ETA: 10s - loss: 0.9844 - acc: 0.77 - ETA: 9s - loss: 0.9893 - acc: 0.7723 - ETA: 9s - loss: 0.9749 - acc: 0.771 - ETA: 9s - loss: 0.9698 - acc: 0.775 - ETA: 8s - loss: 0.9836 - acc: 0.768 - ETA: 8s - loss: 1.0021 - acc: 0.767 - ETA: 7s - loss: 1.0070 - acc: 0.764 - ETA: 7s - loss: 1.0100 - acc: 0.763 - ETA: 7s - loss: 1.0103 - acc: 0.763 - ETA: 6s - loss: 0.9955 - acc: 0.765 - ETA: 6s - loss: 1.0073 - acc: 0.758 - ETA: 6s - loss: 1.0094 - acc: 0.757 - ETA: 5s - loss: 1.0098 - acc: 0.755 - ETA: 5s - loss: 1.0151 - acc: 0.753 - ETA: 4s - loss: 1.0321 - acc: 0.750 - ETA: 4s - loss: 1.0319 - acc: 0.748 - ETA: 4s - loss: 1.0287 - acc: 0.751 - ETA: 3s - loss: 1.0191 - acc: 0.752 - ETA: 3s - loss: 1.0178 - acc: 0.753 - ETA: 3s - loss: 1.0223 - acc: 0.753 - ETA: 2s - loss: 1.0235 - acc: 0.753 - ETA: 2s - loss: 1.0183 - acc: 0.754 - ETA: 1s - loss: 1.0232 - acc: 0.752 - ETA: 1s - loss: 1.0236 - acc: 0.754 - ETA: 1s - loss: 1.0137 - acc: 0.757 - ETA: 0s - loss: 1.0081 - acc: 0.759 - ETA: 0s - loss: 1.0053 - acc: 0.758 - ETA: 0s - loss: 1.0073 - acc: 0.756 - 22s 8ms/step - loss: 1.0082 - acc: 0.7573 - val_loss: 1.0767 - val_acc: 0.7626\n",
      "Epoch 14/15\n",
      "2583/2583 [==============================] - ETA: 15s - loss: 0.7825 - acc: 0.81 - ETA: 14s - loss: 0.8067 - acc: 0.79 - ETA: 14s - loss: 0.8226 - acc: 0.80 - ETA: 13s - loss: 0.8772 - acc: 0.79 - ETA: 13s - loss: 0.8863 - acc: 0.78 - ETA: 12s - loss: 0.8921 - acc: 0.79 - ETA: 12s - loss: 0.8852 - acc: 0.78 - ETA: 12s - loss: 0.9168 - acc: 0.77 - ETA: 11s - loss: 0.9074 - acc: 0.78 - ETA: 11s - loss: 0.9139 - acc: 0.79 - ETA: 10s - loss: 0.9559 - acc: 0.78 - ETA: 10s - loss: 0.9573 - acc: 0.78 - ETA: 10s - loss: 0.9468 - acc: 0.79 - ETA: 9s - loss: 0.9357 - acc: 0.7946 - ETA: 9s - loss: 0.9105 - acc: 0.800 - ETA: 9s - loss: 0.9197 - acc: 0.794 - ETA: 8s - loss: 0.9225 - acc: 0.791 - ETA: 8s - loss: 0.9097 - acc: 0.793 - ETA: 7s - loss: 0.9247 - acc: 0.790 - ETA: 7s - loss: 0.9369 - acc: 0.791 - ETA: 7s - loss: 0.9318 - acc: 0.790 - ETA: 6s - loss: 0.9194 - acc: 0.790 - ETA: 6s - loss: 0.9211 - acc: 0.790 - ETA: 6s - loss: 0.9176 - acc: 0.788 - ETA: 5s - loss: 0.9123 - acc: 0.787 - ETA: 5s - loss: 0.9085 - acc: 0.788 - ETA: 4s - loss: 0.9102 - acc: 0.789 - ETA: 4s - loss: 0.9222 - acc: 0.788 - ETA: 4s - loss: 0.9189 - acc: 0.788 - ETA: 3s - loss: 0.9208 - acc: 0.786 - ETA: 3s - loss: 0.9252 - acc: 0.784 - ETA: 3s - loss: 0.9350 - acc: 0.779 - ETA: 2s - loss: 0.9334 - acc: 0.779 - ETA: 2s - loss: 0.9458 - acc: 0.776 - ETA: 1s - loss: 0.9501 - acc: 0.774 - ETA: 1s - loss: 0.9500 - acc: 0.773 - ETA: 1s - loss: 0.9494 - acc: 0.772 - ETA: 0s - loss: 0.9424 - acc: 0.773 - ETA: 0s - loss: 0.9478 - acc: 0.768 - ETA: 0s - loss: 0.9479 - acc: 0.769 - 22s 8ms/step - loss: 0.9437 - acc: 0.7708 - val_loss: 1.0370 - val_acc: 0.7653\n",
      "Epoch 15/15\n",
      "2583/2583 [==============================] - ETA: 14s - loss: 0.7512 - acc: 0.87 - ETA: 14s - loss: 0.7994 - acc: 0.83 - ETA: 14s - loss: 0.8323 - acc: 0.80 - ETA: 13s - loss: 0.8742 - acc: 0.80 - ETA: 13s - loss: 0.8238 - acc: 0.82 - ETA: 12s - loss: 0.8315 - acc: 0.81 - ETA: 12s - loss: 0.8396 - acc: 0.81 - ETA: 12s - loss: 0.8302 - acc: 0.81 - ETA: 11s - loss: 0.8606 - acc: 0.80 - ETA: 11s - loss: 0.8710 - acc: 0.79 - ETA: 10s - loss: 0.8643 - acc: 0.79 - ETA: 10s - loss: 0.8515 - acc: 0.80 - ETA: 10s - loss: 0.8707 - acc: 0.79 - ETA: 9s - loss: 0.8712 - acc: 0.7946 - ETA: 9s - loss: 0.8679 - acc: 0.797 - ETA: 9s - loss: 0.8883 - acc: 0.795 - ETA: 8s - loss: 0.8815 - acc: 0.796 - ETA: 8s - loss: 0.8801 - acc: 0.795 - ETA: 7s - loss: 0.8759 - acc: 0.795 - ETA: 7s - loss: 0.8708 - acc: 0.795 - ETA: 7s - loss: 0.8624 - acc: 0.796 - ETA: 6s - loss: 0.8685 - acc: 0.794 - ETA: 6s - loss: 0.8798 - acc: 0.793 - ETA: 6s - loss: 0.8681 - acc: 0.794 - ETA: 5s - loss: 0.8677 - acc: 0.796 - ETA: 5s - loss: 0.8719 - acc: 0.797 - ETA: 4s - loss: 0.8855 - acc: 0.792 - ETA: 4s - loss: 0.8970 - acc: 0.790 - ETA: 4s - loss: 0.9017 - acc: 0.787 - ETA: 3s - loss: 0.9005 - acc: 0.786 - ETA: 3s - loss: 0.8969 - acc: 0.787 - ETA: 3s - loss: 0.8968 - acc: 0.786 - ETA: 2s - loss: 0.8909 - acc: 0.787 - ETA: 2s - loss: 0.8819 - acc: 0.789 - ETA: 2s - loss: 0.8849 - acc: 0.787 - ETA: 1s - loss: 0.8810 - acc: 0.787 - ETA: 1s - loss: 0.8827 - acc: 0.786 - ETA: 0s - loss: 0.8817 - acc: 0.785 - ETA: 0s - loss: 0.8821 - acc: 0.786 - ETA: 0s - loss: 0.8873 - acc: 0.784 - 22s 8ms/step - loss: 0.8868 - acc: 0.7832 - val_loss: 1.0125 - val_acc: 0.7608\n"
     ]
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='value_loss')\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "\n",
    "input_text = Input(shape=(1,), dtype=tf.string)\n",
    "custom_layer = KerasLayer(output_dim=1024, trainable=True)(input_text)\n",
    "main_output = Dense(46, activation='softmax', name='main_output')(custom_layer)\n",
    "\n",
    "model = Model(inputs = input_text,  outputs=main_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train_shuf, y_train_shuf, validation_data = (x_test_shuf, y_test_shuf), \\\n",
    "                    batch_size=64, epochs=15, shuffle = True, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test, dtype = str)\n",
    "pred_values = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_classes = pred_values.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6347013409183259"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test.values.argmax(-1), predict_test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_elmo_embeddings(elmo, documents, max_sentences = 1000):\n",
    "    num_sentences = min(max_sentences, len(documents)) if max_sentences > 0 else len(documents)\n",
    "    print(\"\\n\\n:: Lookup of \"+str(num_sentences)+\" ELMo representations. This takes a while ::\")\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    tokens = [document['tokens'] for document in documents]\n",
    "    \n",
    "    documentIdx = 0\n",
    "    for elmo_embedding in elmo.embed_sentences(tokens):  \n",
    "        document = documents[documentIdx]\n",
    "        # Average the 3 layers returned from ELMo\n",
    "        avg_elmo_embedding = np.average(elmo_embedding, axis=0)\n",
    "             \n",
    "        embeddings.append(avg_elmo_embedding)        \n",
    "        labels.append(document['label'])\n",
    "            \n",
    "        # Some progress info\n",
    "        documentIdx += 1\n",
    "        percent = 100.0 * documentIdx / num_sentences\n",
    "        line = '[{0}{1}]'.format('=' * int(percent / 2), ' ' * (50 - int(percent / 2)))\n",
    "        status = '\\r{0:3.0f}%{1} {2:3d}/{3:3d} sentences'\n",
    "        sys.stdout.write(status.format(percent, line, documentIdx, num_sentences))\n",
    "        \n",
    "        if max_sentences > 0 and documentIdx >= max_sentences:\n",
    "input_text = Input(shape=(1,), dtype=tf.string)\n",
    "custom_layer = KerasLayer(output_dim=1024, trainable=True)(input_text)\n",
    "pred = Dense(1, activation='sigmoid', trainable=True)(custom_layer)\n",
    "\n",
    "model = Model(inputs=input_text, outputs=pred)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(train_text, y_train, epochs=15, batch_size=32)            break\n",
    "            \n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-18 16:40:00.82 INFO in 'tensorflow'['tf_logging'] at line 115: Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "tokens_input = [[\"мама\", \"мыла\", \"раму\"], [\"рама\", \"\", \"\"]]\n",
    "tokens_length = [3, 1]\n",
    "embeddings = elmo_model(inputs={\n",
    "                        \"tokens\": tokens_input,\n",
    "                        \"sequence_len\": tokens_length\n",
    "                        },\n",
    "                signature=\"tokens\",\n",
    "                as_dict=True)[\"elmo\"]\n",
    "a = sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toc = data_events['CLEAN_TEXT'].apply(lambda x: x[:45])\n",
    "\n",
    "share = int(len(X_toc)*.6)\n",
    "X_train, Y_train = X_toc[:share], criterias_group[:share]\n",
    "X_test, Y_test = X_toc[share:], criterias_group[share:]\n",
    "\n",
    "\n",
    "\n",
    "x_train_shuf, x_test_shuf, y_train_shuf, y_test_shuf = train_test_split(X_train, Y_train.values, \n",
    "                                                                        test_size = 0.3, random_state = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
